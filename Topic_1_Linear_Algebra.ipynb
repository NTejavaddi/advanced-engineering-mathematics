{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "29ef6a04", "cell_type": "markdown", "source": "# Topic 1: Linear Algebra\n---", "metadata": {}}, {"id": "98013ea2", "cell_type": "markdown", "source": "\n## \ud83d\udcd8 Concept Overview\nLinear Algebra is the branch of mathematics concerning linear equations, matrices, vector spaces, and linear transformations. It is a foundational tool in modern machine learning and deep learning.\n\n---\n", "metadata": {}}, {"id": "4e3191e5", "cell_type": "markdown", "source": "\n## \ud83e\uddee Key Formulas\n\n- Matrix Multiplication:\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\n  \\[ C = A \\cdot B \\]\n\n- Dot Product of Vectors:\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\n  \\[ \u000bec{a} \\cdot \u000bec{b} = \\sum_{i=1}^{n} a_i b_i \\]\n\n- Eigenvalues and Eigenvectors:\n  \\[ A\u000bec{v} = \\lambda \u000bec{v} \\]\n\n- Determinant:\n  \\[ \text{det}(A) \\]\n\n- Inverse of a Matrix:\n  \\[ A^{-1} \text{ such that } A \\cdot A^{-1} = I \\]\n\n---\n", "metadata": {}}, {"id": "4e575ecd", "cell_type": "markdown", "source": "\n## \ud83e\udd16 Applications in AI/ML/DL/NLP\n\n- Representing datasets as matrices (e.g. X for features, y for labels)\n- Linear regression and matrix inversion\n- Word embeddings in NLP (dot product, cosine similarity)\n- Convolution operations in CNNs (matrix operations)\n- PCA (Principal Component Analysis) uses eigenvectors\n\n---\n", "metadata": {}}, {"id": "3b96e39c", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Matrix operations\nA = np.array([[2, 1], [1, 3]])\nB = np.array([[1, 0], [4, 2]])\nC = np.dot(A, B)\nprint(\"Matrix A:\n\", A)\nprint(\"Matrix B:\n\", B)\nprint(\"A @ B = \n\", C)\n\n# Eigenvalues and eigenvectors\neig_vals, eig_vecs = np.linalg.eig(A)\nprint(\"\\nEigenvalues:\", eig_vals)\nprint(\"Eigenvectors:\\n\", eig_vecs)\n\n# Visualizing transformation of a vector\nv = np.array([1, 1])\nAv = A @ v\n\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Original v')\nplt.quiver(0, 0, Av[0], Av[1], angles='xy', scale_units='xy', scale=1, color='red', label='Transformed Av')\nplt.xlim(-1, 5)\nplt.ylim(-1, 5)\nplt.grid()\nplt.legend()\nplt.title(\"Linear Transformation by Matrix A\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.axis('equal')\nplt.show()\n", "outputs": []}, {"id": "72450e85", "cell_type": "markdown", "source": "\n## \ud83d\udd01 Flowchart \u2013 Matrix Multiplication (2D)\n1. Start  \n2. Input matrix A (m\u00d7n) and matrix B (n\u00d7p)  \n3. Initialize result matrix C (m\u00d7p) with zeros  \n4. For each row i in A:  \n   - For each column j in B:  \n     - For each element k:  \n       - Multiply A[i][k] \u00d7 B[k][j] and add to C[i][j]  \n5. Return matrix C  \n6. End\n\n---\n", "metadata": {}}]}